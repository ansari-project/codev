# Codev: Context-First Development

A methodology framework for building software where context drives development, not the other way around.

## Quick Start

Tell your AI agent:
```
Apply Codev to this repo following the instructions at https://github.com/ansari-project/codev/blob/main/INSTALL.md
```

## What is Codev?

Codev is a development methodology that treats **natural language context as code**. Instead of writing code first and documenting later, you start with clear specifications that both humans and AI agents can understand and execute.

### Core Philosophy

1. **Context Drives Code** - Context definitions flow from high-level specifications down to implementation details
2. **Human-AI Collaboration** - Designed for seamless cooperation between developers and AI agents
3. **Evolving Methodology** - The process itself evolves and improves with each project

## The SP(IDE)R Protocol

Our flagship protocol for structured development:

- **S**pecify - Define what to build in clear, unambiguous language
- **P**lan - Break specifications into executable phases
- **For each phase:** **I**mplement â†’ **D**efend â†’ **E**valuate
  - **Implement**: Build the code to meet phase objectives
  - **Defend**: Write comprehensive tests that protect your codeâ€”not just validation, but defensive fortifications against bugs and regressions
  - **Evaluate**: Verify requirements are met, get user approval, then commit
- **R**eview - Capture lessons and improve the methodology

## Project Structure

```
your-project/
â”œâ”€â”€ codev/
â”‚   â”œâ”€â”€ protocols/
â”‚   â”‚   â””â”€â”€ spider/          # The SP(IDE)R protocol
â”‚   â”‚       â”œâ”€â”€ protocol.md  # Detailed protocol documentation
â”‚   â”‚       â”œâ”€â”€ manifest.yaml
â”‚   â”‚       â””â”€â”€ templates/   # Document templates
â”‚   â”œâ”€â”€ specs/              # Feature specifications
â”‚   â”œâ”€â”€ plans/              # Implementation plans
â”‚   â”œâ”€â”€ reviews/            # Review and lessons learned
â”‚   â””â”€â”€ resources/          # Reference materials (llms.txt, etc.)
â”œâ”€â”€ CLAUDE.md               # AI agent instructions
â””â”€â”€ [your code]
```

## Key Features

### ğŸ“„ Documents Are First-Class Citizens
- Specifications, plans, and lessons all tracked
- All decisions captured in version control
- Clear traceability from idea to implementation

### ğŸ¤– AI-Native Workflow
- Structured formats that AI agents understand
- Multi-agent consultation support (GPT-5, Gemini Pro, etc.)
- Reduces back-and-forth from dozens of messages to 3-4 document reviews

### ğŸ”„ Continuous Improvement
- Every project improves the methodology
- Lessons learned feed back into the process
- Templates evolve based on real experience

## ğŸ“š Example Implementations

Both projects below were given **the exact same prompt** to build a Todo Manager application using **Claude Code with Opus**. The difference? The methodology used:

### [Todo Manager - VIBE](https://github.com/ansari-project/todo-manager-vibe)
- Built using a **VIBE-style prompt** approach
- Shows rapid prototyping with conversational AI interaction
- Demonstrates how a simple prompt can drive development
- Results in working code through chat-based iteration

### [Todo Manager - SPIDER](https://github.com/ansari-project/todo-manager-spider)
- Built using the **SPIDER protocol** with full document-driven development
- Same requirements, but structured through formal specifications and plans
- Demonstrates all phases: Specify â†’ Plan â†’ (IDE Loop) â†’ Review
- Complete with specs, plans, and review documents
- Multi-agent consultation throughout the process

### ğŸ“Š Independent Analysis Results

We analyzed both implementations with multiple AI models (GPT-5, Gemini Pro, and deep analysis). Here are the findings:

#### Quality Scores (out of 100)
| Aspect | VIBE | SPIDER |
|--------|------|--------|
| **Overall Score** | **12-15** | **92-95** |
| Functionality | 0 | 100 |
| Test Coverage | 0 | 85 |
| Documentation | 0 | 95 |
| Architecture | N/A | 90 |
| Production Readiness | 0 | 85 |

#### Key Differences

**VIBE Implementation:**
- âŒ **3 files total** - Just Next.js boilerplate
- âŒ **0% functionality** - No todo features implemented
- âŒ **0 tests** - No validation or quality assurance
- âŒ **No database** - No data persistence
- âŒ **No API routes** - No backend functionality
- âŒ **No components** - Just default Next.js template

**SPIDER Implementation:**
- âœ… **32 source files** - Complete application structure
- âœ… **100% functionality** - Full CRUD operations
- âœ… **5 test suites** - API, components, database, MCP coverage
- âœ… **SQLite + Drizzle ORM** - Proper data persistence
- âœ… **Complete API** - RESTful endpoints for all operations
- âœ… **Component architecture** - TodoForm, TodoList, TodoItem, ConversationalInterface
- âœ… **MCP integration** - AI-ready with server wrapper
- âœ… **Type safety** - TypeScript + Zod validation
- âœ… **Error handling** - Boundaries and optimistic updates
- âœ… **Documentation** - Specs, plans, and lessons learned

#### Why SPIDER Won

As GPT-5 noted: *"SPIDER's methodology clearly outperformed... Plan-first approach with defined scope, iterative verification, and delivery mindset"*

Gemini Pro explained: *"SPIDER correctly inferred the user's intent... It saves hours, if not days, of setup... It builds code the way a professional team would"*

The verdict: **Document-driven development ensures completeness**, while conversational approaches can miss the mark entirely despite identical prompts and AI models.

## ğŸ• Eating Our Own Dog Food

Codev is **self-hosted** - we use Codev methodology to build Codev itself. This means:

- **Our test infrastructure** is specified in `codev/specs/0001-test-infrastructure.md`
- **Our development process** follows the SP(IDE)R protocol we advocate
- **Our improvements** come from lessons learned using our own methodology

This self-hosting approach ensures:
1. The methodology is battle-tested on real development
2. We experience the same workflow we recommend to users
3. Any pain points are felt by us first and fixed quickly
4. The framework evolves based on actual usage, not theory

You can see this in practice:
- Check `codev/specs/` for our feature specifications
- Review `codev/plans/` for how we break down work
- Learn from `codev/reviews/` to see what we've discovered

### Test Infrastructure

Our comprehensive test suite (52 tests) validates the Codev installation process:

- **Framework**: Shell-based testing with bats-core (zero dependencies)
- **Coverage**: SPIDER protocol, SPIDER-SOLO variant, CLAUDE.md preservation
- **Isolation**: XDG sandboxing ensures tests never touch real user directories
- **CI/CD Ready**: Tests run in seconds with clear TAP output
- **Multi-Platform**: Works on macOS and Linux without modification

Run tests locally:
```bash
# Fast tests (< 30 seconds)
./scripts/run-tests.sh

# All tests including Claude CLI integration
./scripts/run-all-tests.sh
```

See `tests/README.md` for detailed test documentation.

## Installation

Ask your AI agent to:
```
Install Codev by following the instructions at https://github.com/ansari-project/codev/blob/main/INSTALL.md
```

The agent will:
1. Check for prerequisites (Zen MCP server)
2. Create the codev/ directory structure
3. Install the appropriate protocol (SPIDER or SPIDER-SOLO)
4. Set up or update your CLAUDE.md file

## Examples

### Todo Manager Tutorial

See `examples/todo-manager/` for a complete walkthrough showing:
- How specifications capture all requirements
- How plans break work into phases
- How the IDE loop ensures quality
- How lessons improve future development

## Configuration

### Customizing Templates

Templates in `codev/protocols/spider/templates/` can be modified to fit your team's needs:

- `spec.md` - Specification structure
- `plan.md` - Planning format
- `lessons.md` - Retrospective template

## Protocol Improvement Process

### Spider-Protocol-Updater Agent

When SPIDER implementations in other repositories discover improvements, we can incorporate them back into the main protocol using the `spider-protocol-updater` agent (requires Claude Code with the Task tool):

```bash
# Check a repository for SPIDER improvements
# The agent will analyze their implementation and suggest protocol updates
```

To invoke the agent, ask Claude: "Check [repository-url] for SPIDER improvements"

The agent will:
1. Analyze the repository's SPIDER implementation
2. Compare it against our current protocol
3. Identify valuable improvements and lessons learned
4. Suggest specific protocol updates with justification

Example repositories to monitor:
- `ansari-project/todo-manager-spider` - SPIDER protocol implementation with lessons learned
- Your own SPIDER projects that discovered better patterns

## Contributing

We welcome contributions:
- New protocols beyond SP(IDE)R
- Improved templates
- Integration tools
- Case studies
- SPIDER protocol improvements from your implementations

## License

MIT - See LICENSE file for details

---

*Built with Codev - where context drives code*